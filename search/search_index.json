{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Cluster Usage Guide","text":"<p>This guide introduces:  </p>"},{"location":"#logging","title":"Logging","text":"<p>\u2022   Connecting to a cluster via SSH </p>"},{"location":"#transferring-files","title":"Transferring File(s)","text":"<p>\u2022   Transferring files to/from a cluster with rsync \u2022   Transferring files to/from a cluster with FileZilla &amp; SFTP \u2022   Transferring files to/from a cluster via FTP &amp; curl </p>"},{"location":"#submitting-jobs","title":"Submitting Job(s)","text":"<p>\u2022   Submitting Jobs with sbatch  \u2022   Submitting Interactive Jobs with srun </p>"},{"location":"#monitoring-managing-jobs","title":"Monitoring &amp; Managing Job(s)","text":"<p>\u2022   Monitoring &amp; Managing SLURM Jobs </p>"},{"location":"#running-clean-reproducible-jobs","title":"Running Clean &amp; Reproducible Job(s)","text":"<p>\u2022   Running Clean &amp; Reproducible Jobs with Python \u2022   Running Clean &amp; Reproducible Jobs with R </p>"},{"location":"Interactive_jobs/","title":"Running Interactive Jobs with srun","text":"Table of Content <ul> <li>Running Interactive Jobs with srun<ul> <li>Example: Interactive shell with 4 CPUs and 4 GB RAM for 30 minutes</li> </ul> </li> </ul> <p>srun can be used to run jobs interactively or within a job script. This is useful for quick tests or debugging.</p>"},{"location":"Interactive_jobs/#example-interactive-shell-with-4-cpus-and-4-gb-ram-for-30-minutes","title":"Example: Interactive shell with 4 CPUs and 4 GB RAM for 30 minutes","text":"<p><pre><code>srun --partition=fast --cpus-per-task=4 --mem=4G --time=00:30:00 --pty bash -i\n</code></pre> Or <pre><code>srun -p fast -c 4 -m 4G -t 00:30:00 --pty bash -i\n</code></pre></p> <p>This will open a shell on the compute node <code>fast</code> with the requested resources.</p> <p>Most clusters have different partitions (also called \u201cqueues\u201d) that are optimized for different job durations or resource types. For example:     - fast/short \u2192 for jobs &lt; 24 hours     - long \u2192 for jobs between 24 hours and 30 days     - Other examples might include gpu, highmem, or interactive  </p>"},{"location":"Logging/","title":"Logging into a High-Performance Computing (HPC) Cluster (ssh)","text":"Table of Content <ul> <li>Logging into a High-Performance Computing (HPC) Cluster (ssh)<ul> <li>Access</li> <li>Request your project directory</li> <li>Organising data in the project directory</li> </ul> </li> </ul> <p>High-Performance Computing (HPC) clusters have become indispensable infrastructure in computational biology, addressing the exponential growth in biological data and the computational complexity of modern bioinformatics analyses. As genomic datasets reach terabyte scales and molecular simulations require millions of calculations, traditional desktop computers simply cannot handle the computational demands of contemporary biological research.</p> <p>HPC clusters provide massive parallel processing power by connecting hundreds or thousands of processors, enabling researchers to tackle computationally intensive tasks such as whole-genome assembly, comparative genomics, protein structure prediction, and population genetics studies. Tasks that would take weeks or months on a single computer can be completed in hours or days on a well-configured cluster.</p> <p>Beyond raw computing power, clusters offer specialized resources including high-memory nodes for memory-intensive applications, GPU acceleration for machine learning algorithms, and high-speed interconnects for data-intensive workflows. The scalability of cluster computing allows researchers to adjust computational resources based on project requirements, from small pilot studies to large-scale genomic consortiums.</p> <p>Most importantly, HPC clusters enable collaborative science by providing shared computing environments where multiple researchers can access the same tools, databases, and computational resources. This infrastructure has become essential for advancing fields like precision medicine, evolutionary biology, and systems biology, where computational bottlenecks often limit scientific discovery.</p> <p>In modern computational biology, access to HPC resources is no longer a luxury\u2014it's a fundamental requirement for competitive research.</p>"},{"location":"Logging/#access","title":"Access","text":""},{"location":"Logging/#the-terminal","title":"The Terminal","text":"<p>The terminal serves as a fundamental tool in computational biology, providing researchers with direct access to powerful command-line interfaces essential for bioinformatics workflows. Unlike graphical applications, the terminal enables efficient processing of large biological datasets that are common in genomics, proteomics, and structural biology.</p> <p>Computational biologists rely on the terminal to execute specialized software like BLAST for sequence alignment, samtools for genomic data manipulation, and R or Python scripts for statistical analysis. The command-line environment excels at batch processing multiple files, automating repetitive tasks through scripting, and chaining together different bioinformatics tools via pipes and redirections.</p> <p>For computationally intensive tasks such as whole-genome sequencing analysis, protein folding simulations, or large-scale phylogenetic reconstructions, using a HPC cluster becomes essential. These high-performance computing resources are accessed exclusively through the terminal via SSH connections, making command-line proficiency not just helpful but absolutely critical for modern computational biology.</p> <p>Linux and macOS have a terminal installed by default.</p> macOS Terminal Icon Ubuntu 22.04 Terminal Icon <p> macOS Terminal</p> <p>On Windows, you have to install a terminal emulator like MobaXterm. </p>"},{"location":"Logging/#the-prompt","title":"The prompt","text":"<p>The shell prompt is the command prompt that appears in the terminal, indicating that the system is waiting for input from the user. The <code>$</code> symbol at the end of the prompt is a Unix/Linux convention which indicates that you are logged in as a normal user (non-administrator). On the other hand, the <code>#</code> symbol indicates that you are logged in as root. <pre><code>user@hostname:~$ ls          # User\nroot@hostname:~# systemctl   # Administrator (root)\n</code></pre> The prompt can contain information such as the user name, machine name and current directory, and ends with $ or # depending on privileges. This visual distinction makes it possible to quickly identify the level of access before executing potentially dangerous commands.</p>"},{"location":"Logging/#connection-with-ssh","title":"Connection with ssh","text":"<p>To log on to the cluster, you typically use a secure shell (ssh) from your local machine. Open a new shell and enter the following command:</p> Connexion<pre><code># Function to add two numbers\n$ ssh username@cluster.address.edu\n</code></pre> <p>Replace username with your cluster login name and cluster.address.edu with the actual domain or IP of the cluster. You will be prompted to enter your password, which is usually provided by the cluster administrator.  </p> <p>Extra Tip for Linux / macOS users</p> <p>If you need to connect to the cluster frequently, it may be useful to save the address in an alias, stored in the .bashrc file. The .bashrc file is a configuration script that runs automatically every time you open a new interactive Bash shell session on Unix-like systems. It contains custom settings, aliases, functions, and environment variables that personalize your command-line experience and define how your shell behaves. All you have to do is enter the alias followed by the enter key to access the cluster directly: <pre><code>$ echo \"alias cluster='ssh username@cluster.address.edu'\" &gt;&gt; ~/.bashrc\n$ source ~/.bashrc\n</code></pre> The next time you want to connect to the cluster, simply type the alias name followed by enter: <pre><code>$ cluster\n</code></pre></p> <p> Once connected, you\u2019ll land in your home directory (<code>$HOME</code> | <code>~</code> | <code>/home/username/</code>) on the cluster\u2019s head node or login node. This is where you can prepare your job scripts, transfer files, and monitor jobs. </p>"},{"location":"Logging/#request-your-project-directory","title":"Request your project directory","text":"<p>Some cluster managers propose limited use of the home directory and prefer data to be hosted in a dedicated project directory. In this case, the Principal Investigator (PI) applies to the cluster administrator for a project directory and space (in TeraBytes).</p>"},{"location":"Logging/#organising-data-in-the-project-directory","title":"Organising data in the project directory","text":"<p>Once the project file has been created, it is essential to get the project off to a good start, by deciding on an organisation that must be respected over time.   Choose a structure that works well for you and stick to it as much as possible. Below is a suggested way of organising the code:</p> <pre><code>my_project/\n\u251c\u2500\u2500 Cluster_logs/               \n\u251c\u2500\u2500 code/                       # Script directory\n\u2502   \u2514\u2500\u2500 myscript.sh\n\u2502   \u2514\u2500\u2500 setup_env.R             \n\u251c\u2500\u2500 data/                       # Raw data directory\n\u2502   \u2514\u2500\u2500 data.csv\n\u251c\u2500\u2500 results/                    # Analysis results directory\n\u251c\u2500\u2500 tests/                      # Temporary test code\n\u2502   \u2514\u2500\u2500 my_temporary_script.R\n\u251c\u2500\u2500 tmp/                        \n\u251c\u2500\u2500 .gitignore \n\u2514\u2500\u2500 README.md\n</code></pre> <p>Mandatory: - <code>README.md</code> file. This file is arguably the most important file you\u2019ll create in your project. It should provide a clear overview of the code\u2019s purpose, including:     - The goal of the project     - Set-up and execution instructions     - If no separate documents exist (e.g., architecture diagram or system maintenance guide), a high-level explanation of how the code is structured and works</p> <p>The <code>README.md</code> is not the place to describe individual functions in detail</p> <p>Keep it high-level and accessible.  The <code>README.md</code> is a living document. As your code evolves, make sure to keep the documentation up to date.  Don\u2019t postpone writing it until the end of the project \u2014 by then, you\u2019ll likely be racing against deadlines and already thinking about your next project.  Start documenting early. It will save you time and effort later. </p> <p>File / directories to be created: - <code>Cluster_logs</code> directory. It contains logs files redirected from standard output (<code>.out</code>) and standard error (<code>.err</code>). - <code>code/</code> directory. It contains scripts, in the form of <code>.R</code>, <code>.sh</code>, <code>.py</code> files. - <code>data/</code> directory. It contains data to be analyzed (.csv, .vcf...). - <code>results/</code> directory. It contains outputs. It is recommended to add subdirectories in this directory. - <code>tests/</code> directory. Code tests should go in here. - <code>tmp/</code>directory. It is used to store temporary files generated by the programs or workflows within this project. It serves as a local alternative to the system\u2019s default $TMPDIR, which is often saturated or unreliable on shared cluster environments. - <code>.gitignore</code>. In this file, you add everything you do not want ending up in your remote repository. As a minimum, add <code>tmp/</code> file in there.  </p> <p>To create directories: <code>mkdir -p Cluster_logs data code results tests tmp</code> </p>"},{"location":"Monitoring/","title":"Monitoring and Managing SLURM Jobs","text":"Table of Content <ul> <li>Monitoring and Managing SLURM Jobs<ul> <li>View partitions and node status</li> <li>Check job queue</li> <li>Show job details</li> <li>Cancel a job</li> <li>Check resource usage after job completion</li> </ul> </li> </ul> <p>Once your job is submitted, you can check his status, inspect resource usage, or cancel it using the following commands:</p>"},{"location":"Monitoring/#view-partitions-and-node-status","title":"View partitions and node status","text":"<p><pre><code>sinfo\n</code></pre> Displays the status of available partitions and nodes (availability, max time, etc.). </p>"},{"location":"Monitoring/#check-job-queue","title":"Check job queue","text":"<pre><code>squeue\n</code></pre> <p>Shows all jobs currently pending or running.</p> <p></p> <p><pre><code>squeue -u &lt;username&gt;\n</code></pre> Preferable. Displays jobs submitted by a specific user. Example:</p> <pre><code>squeue -u student1\n</code></pre>"},{"location":"Monitoring/#show-job-details","title":"Show job details","text":"<pre><code>scontrol show job &lt;job_id&gt;\n</code></pre> <p>Displays detailed information about a specific job (resources requested, job state, etc.).</p>"},{"location":"Monitoring/#cancel-a-job","title":"Cancel a job","text":"<pre><code>scancel &lt;job_id&gt;\n</code></pre> <p>Cancels the specified job.</p> <pre><code>scancel -u &lt;username&gt;\n</code></pre> <p>Cancels all jobs submitted by the given user.</p>"},{"location":"Monitoring/#check-resource-usage-after-job-completion","title":"Check resource usage after job completion","text":"<p>Some clusters enable job accounting with:</p> <p><pre><code>sacct -j &lt;job_id&gt;\n</code></pre> Displays job statistics such as runtime, memory usage, and CPU efficiency once the job has finished.</p> <p></p> <p>Some clusters use another command available after loading a module: <pre><code>module load reportseff\nreportseff &lt;job_id&gt;\n</code></pre></p>"},{"location":"R/","title":"Running R scripts on a Cluster (clean &amp; reproducible)","text":"Table of Content <ul> <li>Running R scripts on a Cluster (clean &amp; reproducible)<ul> <li>Organize your code</li> <li>Use renv::init() to create clean renv environment and install necessary packages</li> <li>How to</li> <li>Use --vanilla to launch R scripts: avoid side-effects</li> <li>Example in sbatch mode</li> <li>Example in srun mode</li> </ul> </li> </ul> <p>When launching R scripts on a computing cluster, it is highly recommended to run them in a clean environment to ensure reproducibility and avoid unexpected side effects caused by user-specific configurations. By creating a R projects, you will isolate your work from other projects. Not only can you have project specific files and libraries, but also project specific settings. To do so, you just have to:</p> <ol> <li>Create <code>renv</code> environment,</li> <li>Use <code>--vanilla</code> option when running a script,</li> </ol>"},{"location":"R/#organize-your-code","title":"Organize your code","text":"<p>Up next: organize your code. Choose a structure that works well for you and stick to it as much as possible. Below is a suggested way of organising the code, with some files and directories created automatically:</p> <pre><code>my_project/\n\u251c\u2500\u2500 Cluster_logs/  \n\u251c\u2500\u2500 code/                    # R script directory\n\u2502   \u2514\u2500\u2500 myscript.R\n\u2502   \u2514\u2500\u2500 setup_env.R             # First script to run\n\u251c\u2500\u2500 data/                    # Raw data directory\n\u2502   \u2514\u2500\u2500 data.csv\n\u251c\u2500\u2500 results/               # Analysis results directory\n\u251c\u2500\u2500 renv/                    # renv directory\n\u2502   \u2514\u2500\u2500 activate.R\n\u2502   \u2514\u2500\u2500 library\n\u2502   \u2514\u2500\u2500 local\n\u251c\u2500\u2500 renv.lock                # Frozen dependencies\n\u251c\u2500\u2500 tests/                # Temporary test code\n\u2502   \u2514\u2500\u2500 my_temporary_script.R\n\u251c\u2500\u2500 tmp/    \n\u251c\u2500\u2500 .Renviron               # Local environment variables\n\u251c\u2500\u2500 .Rprofile                   # Starting session instructions\n\u251c\u2500\u2500 .gitignore \n\u2514\u2500\u2500 README.md\n</code></pre> <p>Mandatory:  </p> <ul> <li><code>README.md</code> file. This file is arguably the most important file you\u2019ll create in your project. It should provide a clear overview of the code\u2019s purpose, including:</li> <li>The goal of the project</li> <li>Set-up and execution instructions</li> <li>If no separate documents exist (e.g., architecture diagram or system maintenance guide), a high-level explanation of how the code is structured and works</li> </ul> <p>The <code>README.md</code> is not the place to describe individual functions in detail</p> <p>Keep it high-level and accessible.  The <code>README.md</code> is a living document. As your code evolves, make sure to keep the documentation up to date.  Don\u2019t postpone writing it until the end of the project \u2014 by then, you\u2019ll likely be racing against deadlines and already thinking about your next project.  Start documenting early. It will save you time and effort later.</p> <p>File / directories to be created: </p> <ul> <li><code>Cluster_logs</code> directory. It contains logs files redirected from standard output (<code>.out</code>) and standard error (<code>.err</code>).</li> <li><code>code/</code> directory. It contains scripts, in the form of <code>.R</code> files.  </li> <li><code>data/</code> directory. It contains data to be analyzed (.csv, .vcf...).  </li> <li><code>results/</code> directory. It contains outputs. It is recommended to add subdirectories in this directory.  </li> <li><code>tests/</code> folder. Code tests should go in here.  </li> <li><code>tmp/</code>directory. It is used to store temporary files generated by the programs or workflows within this project. It serves as a local alternative to the system\u2019s default $TMPDIR, which is often saturated or unreliable on shared cluster environments.  </li> <li><code>.gitignore</code>. In this file, you add everything you do not want ending up in your remote repository. As a minimum, add <code>.RProj</code> file in there.  </li> </ul> <p>Automatically created if you followed step 1: </p> <ul> <li><code>renv.lock</code> file, <code>renv/</code> folder and <code>.Rprofile</code> file. This is the virtual environment set up. They will be automatically created by renv when you activate it.  </li> <li><code>.Rproj</code> file. This is an RStudio file containing the project specific settings. Created automatically when you create an R project with RStudio ON Demand (see further).  </li> </ul>"},{"location":"R/#use-renvinit-to-create-clean-renv-environment-and-install-necessary-packages","title":"Use <code>renv::init()</code> to create clean <code>renv</code> environment and install necessary packages","text":"<p><code>renv::init()</code> does several important things:</p> <ol> <li>Creates the renv/ infrastructure in your project:  <ol> <li><code>renv/</code>directory with configuration files  </li> <li><code>Rprofile</code> file that activates <code>renv</code> at start-up  </li> <li><code>renv.lock</code> file (empty at start)  </li> </ol> </li> <li>Isolates the environment: creates a private package library for this project only.  </li> <li>Scans the existing code to automatically detect the packages used (via library(), require() ...)  </li> <li>Installs the detected packages in the project's private library (NOT all, unfortunatelly...)  </li> </ol>"},{"location":"R/#how-to","title":"How to","text":"<ol> <li>If you haven't already done so, go into the project's directory</li> </ol> <pre><code>cd /path/to/my_project/\n</code></pre> <ol> <li>Create a setup_env.R file</li> </ol> <pre><code>vim code/setup_env.R\n</code></pre> <pre><code>#!/usr/bin/env Rscript\n# setup_env.R\n\ncat(\"Setting up R environment...\\n\")\n\n# 1. Install renv first (required before)\nif (!requireNamespace(\"renv\", quietly = TRUE)) {\n    cat(\"Installing renv package...\\n\")\n    install.packages(\"renv\", repos = \"https://cran.r-project.org/\")\n}\n\n# 2. Initialize renv for this projet\ncat(\"Initializing renv environment...\\n\")\nrenv::init()\n\n# 3. List all necessary packages\nrequired_packages &lt;- c(\n    \"data.table\",\n    \"ggplot2\",\n    \"dplyr\",\n    \"parallel\",\n    \"devtools\",     # For development purposes (packages available on GitHub)\n    \"BiocManager\"   # For Bioconductor packages if necessary\n)\n\n# 4. CRAN packages management\ncat(\"Installing CRAN packages...\\n\")\ninstall.packages(required_packages)\n\n# 5. Install additional packages if needed\n# 5.1 Bioconductor packages management\nif (\"BiocManager\" %in% required_packages) {\n    cat(\"Installing Bioconductor packages...\\n\")\n    BiocManager::install(c(\"DESeq2\", \"edgeR\"))  # examples\n}\n\n# 5.2 GitHub packages management\ngithub_packages &lt;- c(  \n    \"petrelharp/local_pca/lostruct\",  # Example of a GitHub package\n    \"petrelharp/templater\"            \n)\nif (length(github_packages) &gt; 0) {\n    cat(\"Installing GitHub packages...\\n\")\n    for (pkg in github_packages) {\n        devtools::install_github(pkg)\n    }\n}\n\n# 6. Final snapshot - Freeze all\ncat(\"Creating final snapshot...\\n\")\nrenv::snapshot()\n\n# 7. Final checkup\ncat(\"Setup completed. Session info:\\n\")\nsessionInfo()\n</code></pre> <p>Save the file and quit vim by pressing escape and entering the following command :</p> <pre><code>:wq\n</code></pre> <p>At this step, you should already have made a list of the packages needed for the analyses. If you need to add packages during the analysis, simply edit this file and run it again.  3. Use interactive mode to setup the environment</p> <pre><code>srun -p fast -c 2 --pty bash -i\nmodule load r/4.3.1\nRscript code/setup_env.R\n</code></pre> <p>To check quickly if the environment is set up correctly, you can run:</p> <pre><code>module load r/4.3.1\nR\n</code></pre> <p>Then, in the R console, run:</p> <pre><code>source(\"renv/activate.R\")\nlibrary(ggplot2)\n</code></pre> <p>If it works in interactive mode, then the SLURM script should work as well!  4. The next move: analyzing data</p> <p><code>Interactive mode</code> When you create or open a R script in the project in interactive mode, simply start it with your isolated environment:</p> <pre><code>#!/usr/bin/env Rscript\n# my_script.R\n\n# Activate the renv environment\ncat(\"Activating renv environment...\\n\")\nsource(\"renv/activate.R\")\n</code></pre> <p>The previously versions of installed packages will then be available. Don't forget to add to the end of your script:</p> <pre><code>renv::deactivate()\n</code></pre> <p><code>renv::deactivate</code> will deactivate the renv environment, allowing you to return to the global R environment. It frees up memory used by project specific package loading and settings, which is particularly useful if you are running multiple scripts in the same R session. It also remove temporary files created during renv session.</p> <p></p> <p><code>Batch mode</code> In a bash script running your R script in batch mode, you have to source the <code>renv</code> environment and <code>restore</code> the packages before running your R script. Here is an example of how to do this:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n...\n####################################################################\n# Load module\n...\n\nexport RENV_PATHS_LIBRARY=\"./renv/library\"\nexport RENV_PATHS_CACHE=\"./renv/staging\"\nexport RENV_CONSENT=TRUE\n\necho \"renv environment variables:\"\necho \"  RENV_PATHS_LIBRARY: $RENV_PATHS_LIBRARY\"\necho \"  RENV_PATHS_CACHE: $RENV_PATHS_CACHE\"\necho \"  RENV_CONSENT: $RENV_CONSENT\"\n\n# Restore the renv environment using the local structure\necho \"Restoring renv environment from local project...\"\nRscript -e \"\n# Configure renv to use local paths\noptions(renv.consent = TRUE)\n\n# Activate renv from the project directory\nsource('renv/activate.R')\n\n# Restore packages\ncat('Starting restore...\\n')\nrenv::restore(prompt = FALSE)\n\ncat('renv status after restore:\\n')\nrenv::status()\n\"\n\n# Run your R script\n...\n</code></pre> <p><code>renv::restore()</code> will restore the packages in the project's private library, ensuring that the script runs with the correct versions of the packages. It reads the lockfile to see exactly which packages and versions were recorded when you last ran <code>renv::snapshot()</code>. This ensures that your script runs with the same package versions every time, which is crucial for reproducibility. It identifies differences and downloads and installs the exact versions specified int the <code>renv.lock</code> file.</p>"},{"location":"R/#use-vanilla-to-launch-r-scripts-avoid-side-effects","title":"Use --vanilla to launch R scripts: avoid side-effects","text":"<p>The <code>--vanilla</code> option disables all user and system configuration files and avoids automatic workspace saving/loading. It is equivalent to:</p> <pre><code>--no-save:       don't save workspace \n--no-restore:   don't restore workspace\n--no-site-file:  ignore system configuration files\n--no-init-file:  ignore .Rprofile\n--no-environ:   ignore .Renviron\n</code></pre> <p>This ensures that your script runs exactly the same way regardless of your personal <code>.Rprofile</code>, <code>.Renviron</code>, or any <code>.RData</code> file. <code>--vanilla</code> does NOT manage versions of packages. That's why <code>renv</code> is necessary.</p>"},{"location":"R/#example-in-sbatch-mode","title":"Example in sbatch mode","text":""},{"location":"R/#create-a-bash-script-to-run-your-r-script-in-a-clean-environment","title":"Create a bash script to run your R script in a clean environment","text":"<p>In your project directory, create a file in charge of restoring the created environment and running the R script:</p> <pre><code>cd /path/to/your/directory/code/\nvim run my_script.sh\n</code></pre> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=phylogenetic_tree\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Load module\nmodule purge\nmodule load r/4.3.1\n\n# Environment variables for renv\nexport RENV_PATHS_LIBRARY=\"./renv/library\"\nexport RENV_PATHS_CACHE=\"./renv/staging\"\nexport RENV_CONSENT=TRUE\n\necho \"renv environment variables:\"\necho \"  RENV_PATHS_LIBRARY: $RENV_PATHS_LIBRARY\"\necho \"  RENV_PATHS_CACHE: $RENV_PATHS_CACHE\"\necho \"  RENV_CONSENT: $RENV_CONSENT\"\n\n# Restore the renv environment using the local structure\necho \"Restoring renv environment from local project...\"\nRscript -e \"\n# Configure renv to use local paths\noptions(renv.consent = TRUE)\n\n# Activate renv from the project directory\nsource('renv/activate.R')\n\n# Check the status of renv\ncat('renv status before restore:\\n')\nrenv::status()\n\n# Restore packages\ncat('Starting restore...\\n')\nrenv::restore(prompt = FALSE)\n\ncat('renv status after restore:\\n')\nrenv::status()\n\"\n\nRscript --vanilla my_script.R &gt; run_${SLURM_JOB_ID}.Rout 2&gt;&amp;1\n\n# Check if the script completed successfully\nif [ $? -eq 0 ]; then\n    echo \"Script completed successfully\"\nelse\n    echo \"Script failed with exit code $?\"\n    echo \"Last few lines of output:\"\n    tail -20 1Ksnp_run1_10nmds_lostruct_${SLURM_JOB_ID}.Rout\nfi\n</code></pre> <p>Save the file and quit the editor (esc + :wq).</p>"},{"location":"R/#make-the-bash-script-executable","title":"Make the bash script executable","text":"<pre><code>chmod +x my_script.sh\n</code></pre>"},{"location":"R/#createedit-your-r-script","title":"Create/Edit your R script","text":"<pre><code>vim my_script.R\n\nIn the script, add the following lines at the beginning:\n```R\n#!/usr/bin/env Rscript\n# my_script.R\n\n# Activate the renv environment\ncat(\"Activating renv environment...\\n\")\nsource(\"renv/activate.R\")\n\n# Check that you are in the correct environment\ncat(\"Library paths:\\n\")\nprint(.libPaths())\n\ncat(\"Loading required packages...\\n\")\nsuppressPackageStartupMessages({\n    library(mypackage) # Replace with your actual package names\n})\n\n# ... rest of your my_script.R script ...\n</code></pre>"},{"location":"R/#running-the-script","title":"Running the script","text":"<p>To run the script, just type the following command:</p> <pre><code>sbatch --vanilla my_script.R\n</code></pre> <p>if <code>--vanilla</code> is already in the bash script (ie: Rscript --vanilla my_script.R), just type:</p> <pre><code>sbatch my_script.R\n</code></pre> <p>This opens a clean R session in the compute node with the specified resources.</p>"},{"location":"R/#example-in-srun-mode","title":"Example in srun mode","text":"<ol> <li>Follow the procedure described previously.</li> <li>Ask for resources in interactive mode</li> </ol> <pre><code>srun --pty --time=2:00:00 --mem=8G bash\n</code></pre> <ol> <li>Create a complete R script  </li> <li>Run the analysis file</li> </ol> <pre><code>srun --time=1:00:00 --mem=4G Rscript --vanilla my_script.R \n</code></pre>"},{"location":"Submitting_jobs/","title":"Submitting a Job with sbatch","text":"Table of Content <ul> <li>Submitting a Job with sbatch<ul> <li>SLURM CONFIGURATION HEADER</li> <li>Summary of the main SLURM directives</li> <li>Submitting a SLURM job on a simple bash script</li> <li>Submitting a SLURM job on a python script</li> <li>Submitting a SLURM job on a R script:</li> <li>Summary</li> </ul> </li> </ul> <p>The <code>sbatch</code> command is used to submit batch jobs to the SLURM scheduler. SLURM (Simple Linux Utility for Resource Management) is a job scheduler widely used in high-performance computing (HPC) clusters. It manages and allocates computing resources, queues user-submitted jobs, and efficiently schedules them across cluster nodes to optimize workload distribution and resource utilization.</p> <p>Jobs submitted via sbatch are defined in a batch script (usually with a .sh or .slurm extension). This script contains a header section specifying the requested resources (such as number of CPUs, memory, runtime), followed by the commands to execute. The scheduler reads these directives and runs the job accordingly.</p> <p>The most common way to launch a job on a SLURM cluster is with a script written in Bash scripting language. </p> <p>In addition to the bash commands it must execute, Bash script must include a header containing the SLURM directives, specifying the resources requested (number of CPUs, memory, maximum time, partition, etc.). </p>"},{"location":"Submitting_jobs/#slurm-configuration-header","title":"SLURM CONFIGURATION HEADER","text":"<p>The SLURM scheduler will read this header to determine the priority of the script in the queue. </p> <p>SLURM does not use a priority based on the order of directives, but calculates a numerical priority for each job, according to several factors (partition requested, time in the queue, size of the job (=number of nodes, memory and cpus requested)). The number of directives entered in the header also has a positive influence on the priority.</p> <p>These directives begin with <code>#SBATCH</code>:</p> <pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=fastqc\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n</code></pre>"},{"location":"Submitting_jobs/#slurm-account-a-account","title":"SLURM Account (-A / --account=)","text":"<p><pre><code>#SBATCH -A invalbo\nor \n#SBATCH --account=invalbo\n</code></pre> An account name must be entered. In most cases, this corresponds to the name of the project submitted and therefore the name of the directory in which the scripts and data to be processed are stored on the cluster.</p>"},{"location":"Submitting_jobs/#time-t-time","title":"Time (-t / --time=)","text":"<p><pre><code>#SBATCH --time=2-23:00:00\n</code></pre> This directive corresponds to the maximum time that must be allocated to the job. The number of days appears first (max. 30), followed by a dash, then the hours:minutes:seconds. If the job exceeds this time, it is automatically cancelled by the scheduler. Note that some partitions, such as fast, do not accept jobs lasting more than 24hrs.</p>"},{"location":"Submitting_jobs/#job-name-j-job-name","title":"Job name (-J / --job-name=)","text":"<p><pre><code>#SBATCH --job-name=fastqc\n</code></pre> Name of the job that appears in SLURM information, available with <code>sinfo</code> or <code>squeue</code></p>"},{"location":"Submitting_jobs/#partition-p-partition","title":"Partition (-p / --partition=)","text":"<p><pre><code>#SBATCH -p long\n</code></pre> Name of the partition to be used (fastc, long, gpu...).</p>"},{"location":"Submitting_jobs/#nodes-n-nodes","title":"Nodes (-N / --nodes)","text":"<p><pre><code>#SBATCH -N 1\n</code></pre> Number of nodes required for the job. A node is a physical machine in the cluster. Using more than one node (-N &gt; 1) in a SLURM job is rare in standard workflows, but essential in certain large-scale distributed computing contexts. </p> <p>You use several nodes when...</p> <ol> <li>You need more cores or memory than a single node can provide</li> <li>You run distributed computing (MPI)</li> </ol> <p>This is the classic case for using several nodes. For example, in numerical simulation, molecular modelling, deep learning on supercomputers, etc.</p> <p><code>-N &gt; 1</code> is rarely used.</p>"},{"location":"Submitting_jobs/#tasks-n-ntasks","title":"Tasks (-n / --ntasks)","text":"<p><pre><code>#SBATCH -n 1\n</code></pre> Total number of tasks (parallel processes) to run. Each task can run on a separate thread or core, or even a separate node. If -n &gt; 1, tells SLURM to run several parallel tasks (often for MPI, or sample processing). </p> <p><code>-n &gt; 1</code> is rarely used.</p>"},{"location":"Submitting_jobs/#cpus-memory-cpus-per-task-m-mem","title":"CPUs &amp; Memory (--cpus-per-task | -m / --mem)","text":"<p><pre><code>#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n</code></pre> <code>--cpus-per-task</code> is a long option only. It is used to specify the number of cores (CPU threads) to allocate per task. <code>--mem=</code> is used to specify the maximum memory allocated to the task.</p>"},{"location":"Submitting_jobs/#slurm-job-array-array","title":"SLURM Job Array (--array)","text":"<p><pre><code>#SBATCH --array=1-22\n</code></pre> means that SLURM will submit 22 identical jobs, each with a unique index accessible via the environment variable <code>SLURM_ARRAY_TASK_ID</code>. This is commonly used to iterate over samples or files and apply the same script automatically. (see <code>SLURM job on a simple bash script</code> for more info). The slurm array range used must correspond to the number of samples to be processed.</p>"},{"location":"Submitting_jobs/#output-error-management-o-output-e-error","title":"Output &amp; Error management (-o / --output= | -e / --error)","text":"<p><pre><code>#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n</code></pre> are used to redirect standard and error output to files. This is very useful for debugging and tracing jobs. <code>-o</code> or <code>--output=</code> defines the file in which the job's standard output (stdout) will be saved, i.e. what would normally be printed on the screen.</p> <p><code>-e</code> or <code>--error=</code> defines the file in which the error output (stderr) will be saved, i.e. all error messages (system errors, execution errors, etc.).</p>"},{"location":"Submitting_jobs/#about-slurm-variables","title":"About SLURM variables:","text":"<p>In an SBATCH directive, certain SLURM variables correspond to : - %x: job name (defined by --job-name or -J) - %j: job ID (unique number assigned by SLURM) - %N: name of the compute node on which the job is running  </p>"},{"location":"Submitting_jobs/#other-common-sbatch-variables","title":"Other common SBATCH variables :","text":"<ul> <li>Job information :  </li> <li><code>%A</code>: main job ID (for job arrays)  </li> <li><code>%a</code>: job array index  </li> <li><code>%u</code>: user name  </li> <li> <p><code>%x</code>: job name  </p> </li> <li> <p>System information :  </p> </li> <li><code>%N</code>: node name (short)  </li> <li><code>%n</code>: relative node name in allocation  </li> <li><code>%t</code>: job ID  </li> </ul>"},{"location":"Submitting_jobs/#mail-mail","title":"Mail (--mail)","text":"<p><pre><code>#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n</code></pre> You can use this directive to enter an email address. If this is the case, SLURM will send an email to warn you if any problems arise during the job.</p> <p><code>--mail-type</code> Main options:</p> <ul> <li>NONE: no mail (default)</li> <li>BEGIN: at the start of the job</li> <li>END: at the end of the job (success or failure)</li> <li>FAIL: only in the event of failure</li> <li>REQUEUE: when the job is queued again</li> <li>ALL: equivalent to BEGIN,END,FAIL,REQUEUE</li> </ul>"},{"location":"Submitting_jobs/#summary-of-the-main-slurm-directives","title":"Summary of the main SLURM directives","text":"<p>Short option (single hyphen -):   - No equal sign (=)   - Arguments are separated by a space  </p> Long Option Short Option Description <code>--mem=</code> <code>-m</code> Total memory per node <code>--time=</code> <code>-t</code> Job time limit <code>--partition=</code> <code>-p</code> Partition (queue) name <code>--cpus-per-task=</code> \u2014 Number of CPUs per task <code>--account=</code> <code>-A</code> Computing project/account <code>--job-name=</code> <code>-J</code> Job name <code>--output=</code> <code>-o</code> Standard output file <code>--error=</code> <code>-e</code> Error log file"},{"location":"Submitting_jobs/#submitting-a-slurm-job-on-a-simple-bash-script","title":"Submitting a SLURM job on a simple bash script","text":"<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=fastqc\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nmodule load fastqc/0.12.1\nmodule load bcftools/1.14\n\n# Recover of fastq file name\nSAMPLELANE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\n\n# Locate INPUT directory \nINPUT_FOLDER=\"raw\"\n\n\nfastqc --threads 8 -o qc/fastqc/ \"$INPUT_FOLDER/${SAMPLELANE}_R1.fastq.gz\"\nfastqc --threads 8 -o qc/fastqc/ \"$INPUT_FOLDER/${SAMPLELANE}_R2.fastq.gz\"\n\nbcftools merge --threads 12 -r 3L $(cat vcf_files.txt) -Oz -o UG.ag3.3L.vcf.gz\n</code></pre> <p>After the SLURM directives, the rest of the script contains regular Bash commands. These commands can use SLURM environment variables or call external programs.</p>"},{"location":"Submitting_jobs/#loading-environment-modules","title":"Loading Environment Modules","text":"<p>Most HPC systems use environment modules to manage software. To ensure the required tools are available (and versions are controlled), modules must be loaded explicitly:</p> <pre><code>module load fastqc/0.12.1\nmodule load bcftools/1.14\n</code></pre> <p>These commands load specific versions of fastqc and bcftools that are pre-installed on the cluster. To find out which programs and program versions are available on the cluster, use the following command:</p> <p><pre><code>module avail\n</code></pre> You can scroll through the long list of available modules using the space bar or the vertical scroll arrows.</p>"},{"location":"Submitting_jobs/#using-slurm-variables-and-automating-input","title":"Using SLURM Variables and Automating Input","text":"<p><pre><code># Recover fastq filename from a list based on the SLURM_ARRAY_TASK_ID\nSAMPLELANE=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" info_files/fastq_files)\n</code></pre> This command retrieves the appropriate sample name from a text file (info_files/fastq_files), where each line corresponds to a sample:</p> <p>Content of the info_files/fastq_files file<pre><code>Sample_10_EKDN230051732-1A_H2F5FDSXC_L4\nSample_11_EKDN230051733-1A_H2WC5DSXC_L1\nSample_13_EKDN230051735-1A_HFTFWDSX7_L4\nSample_14_EKDN230051736-1A_HFTFWDSX7_L4\nSample_15_EKDN230051737-1A_H2F5FDSXC_L4\n</code></pre> The script dynamically selects the correct line based on the job array index. <code>Sample_10_EKDN230051732-1A_H2F5FDSXC_L4</code> will have the index <code>${SLURM_ARRAY_TASK_ID}</code> number 1 and will be ran in the first array, <code>Sample_11_EKDN230051733-1A_H2WC5DSXC_L1</code> will take the index <code>${SLURM_ARRAY_TASK_ID}</code> number 2 and will be ran in the second array... </p>"},{"location":"Submitting_jobs/#defining-input-directory-and-running-the-commands","title":"Defining Input Directory and Running the Commands","text":"<pre><code># Locate INPUT directory \nINPUT_FOLDER=\"raw\"\n</code></pre> <p>In this case, the program FastQC is then run on the paired-end reads: <pre><code>fastqc --threads 8 -o qc/fastqc/ \"$INPUT_FOLDER/${SAMPLELANE}_R1.fastq.gz\"\nfastqc --threads 8 -o qc/fastqc/ \"$INPUT_FOLDER/${SAMPLELANE}_R2.fastq.gz\"\n</code></pre> These commands perform quality control on the forward and reverse FASTQ files of the current sample.</p>"},{"location":"Submitting_jobs/#submitting-a-slurm-job-on-a-python-script","title":"Submitting a SLURM job on a python script","text":"<p>You can submit a python script directly to SLURM: <pre><code>sbatch --wrap=\"python my_script.py\" \\\n       -A invalbo \\\n       --time=2-23:00:00 \\\n       --job-name=python_wrap \\\n       -p long \\\n       --cpus-per-task=8 \\\n       --mem=8G\n</code></pre></p> <p>However, this will have major limitations</p> <ul> <li>No module loading: impossible to make module load python/3.11 in the Python script  </li> <li>Environment variables: more difficult to configure  </li> <li>Error handling: less control over execution  </li> <li>Debugging: more complicated to debug  </li> </ul>"},{"location":"Submitting_jobs/#running-a-simple-python-script","title":"Running a simple python script","text":"<p>In addition to your python script file, you need to prepare in the same directory a Bash script containing:</p> A simple job<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=23:00:00\n#SBATCH --job-name=python_script\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\n# Load python 3.11 module\nmodule load python/3.11\n\n# Run python script\npython my_script.py\n</code></pre> <p>The last command will run the python script. All log messages from this script will be saved in the files defined by <code>-o</code> and <code>-e</code> options.</p>"},{"location":"Submitting_jobs/#running-a-complex-python-script","title":"Running a complex python script","text":"<p>It is possible to retrieve the values of SLURM environment variables and pass them to the script: </p> A complex job script for a complex python script<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=data_processing\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nmodule load python/3.11\n\n# environment variables\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nexport SAMPLE_ID=$SLURM_ARRAY_TASK_ID\n\n# Define pathways\nINPUT_DIR=\"/data/raw/samples\"\nOUTPUT_DIR=\"/data/processed/results\"\nCONFIG_FILE=\"/configs/analysis_config.yaml\"\n\n# Create output directory\nmkdir -p ${OUTPUT_DIR}\n\n# Runnng script with multiple arguments\npython analysis_pipeline.py \\\n    --input ${INPUT_DIR}/sample_${SAMPLE_ID}.fastq \\\n    --output ${OUTPUT_DIR}/sample_${SAMPLE_ID}_results \\\n    --config ${CONFIG_FILE} \\\n    --threads ${SLURM_CPUS_PER_TASK} \\\n    --memory ${SLURM_MEM_PER_NODE} \\\n    --job-id ${SLURM_JOB_ID} \\\n    --array-id ${SLURM_ARRAY_TASK_ID} \\\n    --verbose \\\n    --force-overwrite\n\n# Check output code\nif [ $? -eq 0 ]; then\n    echo \"Job ${SLURM_ARRAY_TASK_ID} completed successfully\"\nelse\n    echo \"Job ${SLURM_ARRAY_TASK_ID} failed with exit code $?\"\n    exit 1\nfi\n</code></pre>"},{"location":"Submitting_jobs/#running-a-script-requiring-a-python-environment","title":"Running a script requiring a Python environment","text":"A complex job script with Python environment<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=ml_training\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nmodule load python/3.11\n\n# Activating virtual environment\nsource /path/to/venv/bin/activate\n\n# Install dependancies if necessary\n# pip install -r requirements.txt\n\n# Job array variable\nPARAM_FILE=\"parameters_${SLURM_ARRAY_TASK_ID}.txt\"\n\npython train_model.py \\\n    --param-file ${PARAM_FILE} \\\n    --output-dir results/run_${SLURM_ARRAY_TASK_ID} \\\n    --n-jobs ${SLURM_CPUS_PER_TASK}\n</code></pre>"},{"location":"Submitting_jobs/#submitting-a-slurm-job-on-a-r-script","title":"Submitting a SLURM job on a R script:","text":""},{"location":"Submitting_jobs/#running-a-simple-rscript","title":"Running a simple Rscript:","text":"A simple Rscript<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=r_analysis\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nmodule load R/4.3.0\n\n# Lancement simple du script R\nRscript my_script.R\n</code></pre>"},{"location":"Submitting_jobs/#running-a-complex-rscript","title":"Running a complex Rscript:","text":"A complex Bash script for a complex Rscript<pre><code>#!/bin/bash\n###################configuration slurm##############################\n#SBATCH -A invalbo\n#SBATCH --time=2-23:00:00\n#SBATCH --job-name=phylogenetic_tree\n#SBATCH -p long\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --cpus-per-task 8\n#SBATCH --mem=8G\n#SBATCH --array 1-22\n#SBATCH -o Cluster_logs/%x-%j-%N.out\n#SBATCH -e Cluster_logs/%x-%j-%N.err\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n####################################################################\n\nmodule load R/4.3.0\n\n# Configuration de l'environnement R\nexport R_LIBS_USER=\"/home/user/R/library\"\nexport TMPDIR=\"/tmp/${SLURM_JOB_ID}\"\nmkdir -p $TMPDIR\n\n# Installation conditionnelle de packages (si n\u00e9cessaire)\nRscript -e \"\nif (!require('BiocManager', quietly = TRUE)) {\n    install.packages('BiocManager', lib = Sys.getenv('R_LIBS_USER'))\n}\nif (!require('phyloseq', quietly = TRUE)) {\n    BiocManager::install('phyloseq', lib = Sys.getenv('R_LIBS_USER'))\n}\n\"\n\n# Variables pour le script\nDATASET=\"dataset_${SLURM_ARRAY_TASK_ID}\"\nCONFIG_FILE=\"/configs/phylo_config.yaml\"\n\n# Lancement avec lecture depuis stdin et redirection\nRscript phylogenetic_analysis.R \\\n    --dataset \"${DATASET}\" \\\n    --config \"${CONFIG_FILE}\" \\\n    --output-prefix \"results/phylo_${SLURM_ARRAY_TASK_ID}\" \\\n    --bootstrap 1000 \\\n    --method \"neighbor-joining\" \\\n    --distance \"bray-curtis\" \\\n    --cores ${SLURM_CPUS_PER_TASK} \\\n    --seed 12345 \\\n    2&gt;&amp;1 | tee \"logs/phylo_${SLURM_ARRAY_TASK_ID}.log\"\n\n# Nettoyage\nrm -rf $TMPDIR\n</code></pre>"},{"location":"Submitting_jobs/#summary","title":"Summary","text":"<pre><code>\u2022   The SLURM header sets up the job environment and resource requirements.  \n\u2022   Modules must be explicitly loaded to ensure required tools are available.  \n\u2022   SLURM_ARRAY_TASK_ID allows you to automate jobs over multiple input samples.  \n\u2022   Output and error logs are stored in the specified Cluster_logs/ directory, with informative file names based on job name and ID.  \n\u2022   To run a python or a R script, use a bash script starting with a SLURM header.\n</code></pre>"},{"location":"python/","title":"Running Python Scripts on a Cluster (Clean and Reproducible)","text":"Table of Content <ul> <li>Running Python Scripts on a Cluster (Clean and Reproducible)<ul> <li>Organize your code</li> <li>Best Practices for Running Python Jobs</li> <li>SLURM Job Script for Python</li> <li>Run a job interactively with srun</li> </ul> </li> </ul> <p>Python is widely used for data science, bioinformatics, and HPC workflows. On a cluster, it\u2019s essential to manage environments carefully to avoid dependency conflicts and ensure reproducibility.</p>"},{"location":"python/#organize-your-code","title":"Organize your code","text":"<p>Read this first: Organizing data in the project directory</p> <p>Below is a suggested way of organizing the code, with some files and directories created automatically: <pre><code>my_project/\n\u251c\u2500\u2500 Cluster_logs/  \n\u251c\u2500\u2500 code/                       # Script directory\n\u2502   \u2514\u2500\u2500 myscript.py\n\u251c\u2500\u2500 data/                       # Raw data directory\n\u2502   \u2514\u2500\u2500 data.csv\n\u251c\u2500\u2500 results/                    # Analysis results directory\n\u251c\u2500\u2500 tests/                      # Temporary test code\n\u2502   \u2514\u2500\u2500 my_temporary_script.py\n\u251c\u2500\u2500 tmp/    \n\u251c\u2500\u2500 venv/                       # Python virtual env.\n\u2502   \u2514\u2500\u2500 bin/\n\u2502         \u2514\u2500\u2500 activate\n\u2502   \u2514\u2500\u2500 include/\n\u2502   \u2514\u2500\u2500 lib/\n\u2502   \u2514\u2500\u2500 pyenv.cfg\n\u2514\u2500\u2500 README.md\n</code></pre></p>"},{"location":"python/#best-practices-for-running-python-jobs","title":"Best Practices for Running Python Jobs","text":"<p>Before launching a job, make sure your script runs in a controlled Python environment.</p>"},{"location":"python/#with-venv-standard-python","title":"With venv (standard Python)","text":"<pre><code>python -m venv myenv\nsource myenv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>the file requirements.txt contains, for example: <pre><code>numpy==1.26.4\npandas&gt;=2.2.0\nscipy&lt;1.12\nmatplotlib\nseaborn==0.12.2\nscikit-learn&gt;=1.3\njupyterlab\nrequests\nbiopython\n</code></pre></p>"},{"location":"python/#with-conda","title":"With conda:","text":"<pre><code>conda create -n myenv python=3.11\nconda activate myenv\nconda install numpy pandas scipy  # etc.\n</code></pre> <p>Once set up, activate your environment inside your SLURM job script before running your script.</p>"},{"location":"python/#slurm-job-script-for-python","title":"SLURM Job Script for Python","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_analysis\n#SBATCH --output=out_%j.log\n#SBATCH --error=err_%j.log\n#SBATCH --time=02:00:00\n#SBATCH --partition=short\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8G\n#SBATCH --account=my_account_name\n\n# Load the Python module (if available)\nmodule load python/3.11\n\n# Optional: Activate your virtual or conda environment\n# For venv\nsource ~/myenv/bin/activate\n# Or for conda\n# source ~/.bashrc\n# conda activate myenv\n\n# Run the script\npython my_script.py\n</code></pre>"},{"location":"python/#run-a-job-interactively-with-srun","title":"Run a job interactively with srun","text":"<p>If you want to test your Python code interactively on a compute node:</p> <pre><code>srun -c 4 -m 8G -t 01:00:00 -p fast -A my_account_name --pty bash -i\n</code></pre> <p>Then inside the shell:</p> <p><pre><code>module load python/3.11\nsource ~/myenv/bin/activate\npython\n</code></pre> Or <pre><code>module load conda\nconda activate myenv\npython\n</code></pre></p>"},{"location":"transfer/FTP/","title":"Downloading Files via FTP with curl","text":""},{"location":"transfer/FTP/#overview","title":"Overview","text":"<p>This guide covers automated downloading of files from FTP servers using <code>curl</code> in HPC environments. The approach demonstrated here is particularly useful for downloading large genomic datasets that require robust error handling and parallel processing.</p>"},{"location":"transfer/FTP/#key-features","title":"Key Features","text":"<ul> <li>Parallel Downloads: Uses SLURM job arrays to download multiple files simultaneously</li> <li>Error Handling: Comprehensive retry mechanisms and integrity checks</li> <li>Progress Monitoring: Detailed logging and progress tracking</li> <li>File Validation: Automatic verification of downloaded files</li> <li>Resume Capability: Skips already downloaded files</li> </ul>"},{"location":"transfer/FTP/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an HPC cluster with SLURM</li> <li><code>curl</code> command-line tool</li> <li>Basic understanding of bash scripting</li> </ul>"},{"location":"transfer/FTP/#script-components","title":"Script Components","text":""},{"location":"transfer/FTP/#slurm-configuration","title":"SLURM Configuration","text":"<p>The script uses SLURM job arrays to parallelize downloads:</p> <pre><code>#SBATCH --array=1-15    # Download 15 files in parallel\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4G\n#SBATCH --time=06:00:00\n</code></pre>"},{"location":"transfer/FTP/#file-management","title":"File Management","text":"<p>Files are organized in arrays with proper indexing:</p> <pre><code>declare -a FILES=(\n    \"\"  # Index 0 is empty to align with SLURM index (1-15)\n    \"ag1000g.phase2.ar1.pass.2L.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.2R.vcf.gz\"\n    # ... more files\n)\n</code></pre>"},{"location":"transfer/FTP/#download-logic","title":"Download Logic","text":"<p>The script implements several safety checks:</p> <ol> <li>Connection Test: Verifies FTP server accessibility</li> <li>File Existence: Checks if files are already downloaded</li> <li>Integrity Verification: Validates VCF.gz files using <code>gzip -t</code></li> <li>Progress Tracking: Logs download progress and duration</li> </ol>"},{"location":"transfer/FTP/#complete-script","title":"Complete Script","text":"<pre><code>#!/bin/bash\n################### SLURM configuration ##############################\n#SBATCH -A invalbo\n#SBATCH -p fast\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH --job-name=ag1000g_download\n#SBATCH --output=logs/ag1000g_download_%A_%a.out\n#SBATCH --error=logs/ag1000g_download_%A_%a.err\n#SBATCH --time=06:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4G\n#SBATCH --array=1-15\n#SBATCH --mail-user=loic.talignani@ird.fr\n#SBATCH --mail-type=FAIL\n################### SLURM configuration ##############################\n\n# Configuration\nFTP_SERVER=\"ngs.sanger.ac.uk\"\nREMOTE_DIR=\"/production/ag1000g/phase2/AR1/variation/main/vcf/pass\"\nLOCAL_DIR=\"./multiallelic\"\nLOG_DIR=\"./logs\"\nLOG_FILE=\"$LOG_DIR/download_log_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.log\"\n\n# Create necessary directories\nmkdir -p \"$LOCAL_DIR\" \"$LOG_DIR\"\n\n# Download file list (index starts from 1)\ndeclare -a FILES=(\n    \"\"  # Index 0 is empty to align with SLURM index (1-15)\n    \"ag1000g.phase2.ar1.pass.2L.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.2R.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.3L.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.3R.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.X.vcf.gz\"\n    \"ag1000g.phase2.ar1.pass.2L.vcf.gz.csi\"\n    \"ag1000g.phase2.ar1.pass.2L.vcf.gz.tbi\"\n    \"ag1000g.phase2.ar1.pass.2R.vcf.gz.csi\"\n    \"ag1000g.phase2.ar1.pass.2R.vcf.gz.tbi\"\n    \"ag1000g.phase2.ar1.pass.3L.vcf.gz.csi\"\n    \"ag1000g.phase2.ar1.pass.3L.vcf.gz.tbi\"\n    \"ag1000g.phase2.ar1.pass.3R.vcf.gz.csi\"\n    \"ag1000g.phase2.ar1.pass.3R.vcf.gz.tbi\"\n    \"ag1000g.phase2.ar1.pass.X.vcf.gz.csi\"\n    \"ag1000g.phase2.ar1.pass.X.vcf.gz.tbi\"\n)\n\n# Logging function\nlog_message() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] [Task ${SLURM_ARRAY_TASK_ID}] $1\" | tee -a \"$LOG_FILE\"\n}\n\n# Get the file to download for this task\nCURRENT_FILE=\"${FILES[$SLURM_ARRAY_TASK_ID]}\"\n\nif [[ -z \"$CURRENT_FILE\" ]]; then\n    log_message \"ERROR: No file assigned to task $SLURM_ARRAY_TASK_ID\"\n    exit 1\nfi\n\nlog_message \"=========================================\"\nlog_message \"STARTING TASK: $SLURM_ARRAY_TASK_ID\"\nlog_message \"Job Array ID: $SLURM_ARRAY_JOB_ID\"\nlog_message \"File to Download: $CURRENT_FILE\"\nlog_message \"FTP Server: $FTP_SERVER\"\nlog_message \"Remote Directory: $REMOTE_DIR\"\nlog_message \"Local Directory: $LOCAL_DIR\"\n\n# Test FTP connection\nlog_message \"Testing FTP connection...\"\nif ! curl -s --connect-timeout 30 \"ftp://$FTP_SERVER/\" &gt; /dev/null 2&gt;&amp;1; then\n    log_message \"ERROR: Unable to connect to FTP server\"\n    exit 1\nfi\nlog_message \"FTP connection successful\"\n\n# Check if the file is already downloaded\nif [[ -f \"$LOCAL_DIR/$CURRENT_FILE\" ]]; then\n    log_message \"File already exists, checking integrity...\"\n\n    # Check if file is non-empty\n    if [[ -s \"$LOCAL_DIR/$CURRENT_FILE\" ]]; then\n        log_message \"File already downloaded and non-empty\"\n\n        # For VCF files, check validity\n        if [[ \"$CURRENT_FILE\" == *.vcf.gz ]]; then\n            if gzip -t \"$LOCAL_DIR/$CURRENT_FILE\" 2&gt;/dev/null; then\n                log_message \"\u2713 VCF.gz is valid, no download needed\"\n                exit 0\n            else\n                log_message \"\u26a0 VCF.gz is corrupted, re-downloading...\"\n                rm -f \"$LOCAL_DIR/$CURRENT_FILE\"\n            fi\n        else\n            log_message \"\u2713 Index file is valid, no download needed\"\n            exit 0\n        fi\n    else\n        log_message \"Empty file detected, removing it...\"\n        rm -f \"$LOCAL_DIR/$CURRENT_FILE\"\n    fi\nfi\n\n# Begin download\nlog_message \"Starting download: $CURRENT_FILE\"\n\n# Adjust timeout based on file type\nif [[ \"$CURRENT_FILE\" == *.vcf.gz ]]; then\n    TIMEOUT=7200  # 2 hours for VCF files\n    log_message \"VCF file detected, timeout set to 2 hours\"\nelse\n    TIMEOUT=1800  # 30 minutes for index files\n    log_message \"Index file detected, timeout set to 30 minutes\"\nfi\n\n# Start download with curl\nSTART_TIME=$(date +%s)\nlog_message \"Command: curl -f -s -S --retry 5 --retry-delay 30 --connect-timeout 60 --max-time $TIMEOUT\"\n\nif curl -f -s -S --retry 5 --retry-delay 30 \\\n    --connect-timeout 60 --max-time $TIMEOUT \\\n    --progress-bar \\\n    -o \"$LOCAL_DIR/$CURRENT_FILE\" \\\n    \"ftp://$FTP_SERVER$REMOTE_DIR/$CURRENT_FILE\" 2&gt;&amp;1 | tee -a \"$LOG_FILE\"; then\n\n    END_TIME=$(date +%s)\n    DURATION=$((END_TIME - START_TIME))\n    FILE_SIZE=$(du -h \"$LOCAL_DIR/$CURRENT_FILE\" | cut -f1)\n\n    log_message \"\u2713 Download successful: $CURRENT_FILE\"\n    log_message \"  Size: $FILE_SIZE\"\n    log_message \"  Duration: ${DURATION}s\"\n\n    # Post-download check for VCF files\n    if [[ \"$CURRENT_FILE\" == *.vcf.gz ]]; then\n        log_message \"Verifying VCF file integrity...\"\n        if gzip -t \"$LOCAL_DIR/$CURRENT_FILE\" 2&gt;/dev/null; then\n            log_message \"\u2713 VCF.gz file is valid\"\n        else\n            log_message \"\u2717 VCF.gz file is corrupted after download\"\n            rm -f \"$LOCAL_DIR/$CURRENT_FILE\"\n            exit 1\n        fi\n    fi\n\n    log_message \"=========================================\"\n    log_message \"TASK $SLURM_ARRAY_TASK_ID COMPLETED SUCCESSFULLY\"\n    exit 0\n\nelse\n    log_message \"\u2717 Download failed: $CURRENT_FILE\"\n    log_message \"curl error code: $?\"\n\n    # Delete partially downloaded file\n    if [[ -f \"$LOCAL_DIR/$CURRENT_FILE\" ]]; then\n        log_message \"Deleting partially downloaded file\"\n        rm -f \"$LOCAL_DIR/$CURRENT_FILE\"\n    fi\n\n    log_message \"=========================================\"\n    log_message \"TASK $SLURM_ARRAY_TASK_ID FAILED\"\n    exit 1\nfi\n</code></pre>"},{"location":"transfer/FTP/#usage-instructions","title":"Usage Instructions","text":""},{"location":"transfer/FTP/#1-prepare-the-script","title":"1. Prepare the Script","text":"<ol> <li>Save the script as <code>download_ag1000g.sh</code></li> <li>Make it executable: <code>chmod +x download_ag1000g.sh</code></li> <li>Create the logs directory: <code>mkdir -p logs</code></li> </ol>"},{"location":"transfer/FTP/#2-submit-the-job","title":"2. Submit the Job","text":"<pre><code>sbatch download_ag1000g.sh\n</code></pre>"},{"location":"transfer/FTP/#3-monitor-progress","title":"3. Monitor Progress","text":"<p>Check job status: <pre><code>squeue -u your_username\n</code></pre></p> <p>View logs: <pre><code>tail -f logs/download_log_*.log\n</code></pre></p>"},{"location":"transfer/FTP/#key-curl-options-explained","title":"Key curl Options Explained","text":"Option Purpose <code>-f</code> Fail silently on server errors <code>-s</code> Silent mode (no progress bar in stdout) <code>-S</code> Show errors even in silent mode <code>--retry 5</code> Retry up to 5 times on failure <code>--retry-delay 30</code> Wait 30 seconds between retries <code>--connect-timeout 60</code> Connection timeout in seconds <code>--max-time 7200</code> Maximum time for the entire operation <code>--progress-bar</code> Show simple progress bar"},{"location":"transfer/FTP/#best-practices","title":"Best Practices","text":""},{"location":"transfer/FTP/#error-handling","title":"Error Handling","text":"<ul> <li>Always test FTP connectivity before downloads</li> <li>Implement file integrity checks</li> <li>Clean up partially downloaded files on failure</li> <li>Use appropriate timeouts for different file types</li> </ul>"},{"location":"transfer/FTP/#logging","title":"Logging","text":"<ul> <li>Include timestamps in all log messages</li> <li>Log both successes and failures</li> <li>Track download duration and file sizes</li> <li>Use unique log files per job/task</li> </ul>"},{"location":"transfer/FTP/#resource-management","title":"Resource Management","text":"<ul> <li>Set appropriate memory and CPU limits</li> <li>Use job arrays for parallel downloads</li> <li>Implement resume capability to avoid re-downloading</li> </ul>"},{"location":"transfer/FTP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"transfer/FTP/#common-issues","title":"Common Issues","text":"<p>Connection Timeouts - Increase <code>--connect-timeout</code> value - Check network connectivity - Verify FTP server availability</p> <p>File Corruption - Implement post-download verification - Use <code>--retry</code> options for transient errors - Check available disk space</p> <p>Permission Errors - Verify write permissions in target directory - Check SLURM account and partition access - Ensure proper file ownership</p>"},{"location":"transfer/FTP/#log-analysis","title":"Log Analysis","text":"<p>Monitor download progress by examining log files:</p> <pre><code># Check overall progress\ngrep \"COMPLETED SUCCESSFULLY\" logs/download_log_*.log | wc -l\n\n# Check for failures\ngrep \"FAILED\" logs/download_log_*.log\n\n# View download statistics\ngrep \"Duration\" logs/download_log_*.log\n</code></pre>"},{"location":"transfer/FTP/#conclusion","title":"Conclusion","text":"<p>This approach provides a robust framework for downloading large datasets from FTP servers in HPC environments. The combination of SLURM job arrays, comprehensive error handling, and file verification ensures reliable data acquisition for genomic analyses.</p> <p>For additional customization, modify the file list, server configuration, and validation logic according to your specific requirements.</p>"},{"location":"transfer/Rsync/","title":"Transfering Files using Rsync","text":"Table of Content <ul> <li>Transfering Files using Rsync<ul> <li>Overview</li> <li>Prerequisites</li> <li>Installation</li> <li>Basic Syntax</li> <li>Essential Options</li> <li>Common Transfer Scenarios</li> <li>Advanced Examples</li> <li>Platform-Specific Examples</li> <li>Best Practices</li> <li>Troubleshooting</li> <li>Monitoring Transfer Progress</li> <li>Security Considerations</li> <li>Conclusion</li> </ul> </li> </ul> <p>Rsync is a powerful and efficient tool for transferring and synchronizing files between your local computer and a remote cluster. It offers several advantages over basic <code>scp</code> or <code>sftp</code>, including incremental transfers, compression, and the ability to resume interrupted transfers.</p>"},{"location":"transfer/Rsync/#overview","title":"Overview","text":"<p>Rsync (Remote Sync) is particularly useful for:</p> <ul> <li>Incremental backups: Only transfers changed files</li> <li>Large file transfers: Can resume interrupted transfers</li> <li>Directory synchronization: Keeps local and remote directories in sync</li> <li>Bandwidth optimization: Built-in compression reduces transfer time</li> </ul>"},{"location":"transfer/Rsync/#prerequisites","title":"Prerequisites","text":"<p>Before using rsync, ensure you have:</p> <ul> <li>SSH access to the cluster</li> <li>Rsync installed on your local machine</li> <li>Your cluster username and hostname</li> </ul>"},{"location":"transfer/Rsync/#installation","title":"Installation","text":""},{"location":"transfer/Rsync/#windows","title":"Windows","text":"<p>Option 1: WSL (Windows Subsystem for Linux) <pre><code># Install WSL first, then in WSL terminal:\nsudo apt update\nsudo apt install rsync\n</code></pre></p> <p>Option 2: Git Bash <pre><code># Rsync comes pre-installed with Git for Windows\n# Download from: https://git-scm.com/download/win\n</code></pre></p> <p>Option 3: Cygwin <pre><code># Install Cygwin and select rsync package during installation\n# Download from: https://www.cygwin.com/\n</code></pre></p>"},{"location":"transfer/Rsync/#linux","title":"Linux","text":"<pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install rsync\n\n# CentOS/RHEL/Fedora\nsudo yum install rsync\n# or for newer versions:\nsudo dnf install rsync\n\n# Arch Linux\nsudo pacman -S rsync\n</code></pre>"},{"location":"transfer/Rsync/#macos","title":"macOS","text":"<pre><code># Rsync is pre-installed on macOS\n# To get the latest version via Homebrew:\nbrew install rsync\n</code></pre>"},{"location":"transfer/Rsync/#basic-syntax","title":"Basic Syntax","text":"<p>The general syntax for rsync is:</p> <pre><code>rsync [OPTIONS] SOURCE DESTINATION\n</code></pre> <p>For cluster transfers: <pre><code># Upload to cluster\nrsync [OPTIONS] /local/path/ username@cluster.example.com:/remote/path/\n\n# Download from cluster\nrsync [OPTIONS] username@cluster.example.com:/remote/path/ /local/path/\n</code></pre></p>"},{"location":"transfer/Rsync/#essential-options","title":"Essential Options","text":"Option Description <code>-a</code> Archive mode (preserves permissions, timestamps, etc.) <code>-v</code> Verbose output <code>-z</code> Compress data during transfer <code>-P</code> Show progress and keep partial files <code>-r</code> Recursive (for directories) <code>-u</code> Update only (skip files that are newer on destination) <code>-n</code> Dry run (preview what would be transferred) <code>--delete</code> Delete files on destination that don't exist on source <code>--exclude</code> Exclude files/directories matching pattern <code>--progress</code> Print information showing the progress of the transfer"},{"location":"transfer/Rsync/#common-transfer-scenarios","title":"Common Transfer Scenarios","text":""},{"location":"transfer/Rsync/#1-upload-a-single-file","title":"1. Upload a Single File","text":"<pre><code># Basic file upload\nrsync -avz myfile.txt username@cluster.example.com:~/\n\n# Upload to specific directory\nrsync -avz myfile.txt username@cluster.example.com:~/data/\n</code></pre>"},{"location":"transfer/Rsync/#2-upload-a-directory","title":"2. Upload a Directory","text":"<pre><code># Upload entire directory (creates 'myproject' on cluster)\nrsync -avz myproject/ username@cluster.example.com:~/myproject/\n\n# Upload directory contents only\nrsync -avz myproject/ username@cluster.example.com:~/existing_dir/\n</code></pre>"},{"location":"transfer/Rsync/#3-download-from-cluster","title":"3. Download from Cluster","text":"<pre><code># Download a file\nrsync -avz username@cluster.example.com:~/results/output.txt ./\n\n# Download a directory\nrsync -avz username@cluster.example.com:~/results/ ./local_results/\n</code></pre>"},{"location":"transfer/Rsync/#4-synchronize-directories","title":"4. Synchronize Directories","text":"<pre><code># Keep local and remote directories in sync\nrsync -avz --delete myproject/ username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#advanced-examples","title":"Advanced Examples","text":""},{"location":"transfer/Rsync/#large-dataset-transfer-with-progress","title":"Large Dataset Transfer with Progress","text":"<pre><code># Transfer large files with detailed progress\nrsync -avzP --human-readable \\\n    /path/to/large/dataset/ \\\n    username@cluster.example.com:~/data/\n</code></pre>"},{"location":"transfer/Rsync/#selective-file-transfer","title":"Selective File Transfer","text":"<pre><code># Only transfer specific file types\nrsync -avz --include=\"*.R\" --include=\"*.py\" --exclude=\"*\" \\\n    myproject/ \\\n    username@cluster.example.com:~/scripts/\n\n# Exclude certain directories\nrsync -avz --exclude=\"*.git\" --exclude=\"__pycache__\" \\\n    myproject/ \\\n    username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#resume-interrupted-transfer","title":"Resume Interrupted Transfer","text":"<pre><code># Use -P to resume partial transfers\nrsync -avzP myproject/ username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#dry-run-before-transfer","title":"Dry Run Before Transfer","text":"<pre><code># Preview what will be transferred (recommended for large transfers)\nrsync -avzn myproject/ username@cluster.example.com:~/myproject/\n\n# If satisfied, remove -n to execute\nrsync -avz myproject/ username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#platform-specific-examples","title":"Platform-Specific Examples","text":""},{"location":"transfer/Rsync/#windows-wslgit-bash","title":"Windows (WSL/Git Bash)","text":"<pre><code># From Windows file system to cluster\nrsync -avz /c/Users/YourName/Documents/myproject/ \\\n    username@cluster.example.com:~/myproject/\n\n# Using WSL paths\nrsync -avz /mnt/c/Users/YourName/Documents/myproject/ \\\n    username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#linux_1","title":"Linux","text":"<pre><code># Standard Linux paths\nrsync -avz ~/Documents/myproject/ \\\n    username@cluster.example.com:~/myproject/\n\n# From external drive\nrsync -avz /media/usb/myproject/ \\\n    username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#macos_1","title":"macOS","text":"<pre><code># From Documents folder\nrsync -avz ~/Documents/myproject/ \\\n    username@cluster.example.com:~/myproject/\n\n# From external drive\nrsync -avz /Volumes/ExternalDrive/myproject/ \\\n    username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#best-practices","title":"Best Practices","text":""},{"location":"transfer/Rsync/#1-use-ssh-keys","title":"1. Use SSH Keys","text":"<p>Set up SSH key authentication to avoid password prompts:</p> <pre><code># Generate SSH key (if not already done)\nssh-keygen -t rsa -b 4096\n\n# Copy public key to cluster\nssh-copy-id username@cluster.example.com\n</code></pre>"},{"location":"transfer/Rsync/#2-create-a-transfer-script","title":"2. Create a Transfer Script","text":"<p>Create a shell script for frequent transfers:</p> <pre><code>#!/bin/bash\n# upload_to_cluster.sh\n\nPROJECT_DIR=\"~/Documents/myproject\"\nCLUSTER_USER=\"username\"\nCLUSTER_HOST=\"cluster.example.com\"\nCLUSTER_PATH=\"~/myproject\"\n\necho \"Starting transfer to cluster...\"\nrsync -avzP \\\n    --exclude=\".git\" \\\n    --exclude=\"__pycache__\" \\\n    --exclude=\"*.tmp\" \\\n    \"$PROJECT_DIR/\" \\\n    \"$CLUSTER_USER@$CLUSTER_HOST:$CLUSTER_PATH/\"\n\necho \"Transfer completed!\"\n</code></pre>"},{"location":"transfer/Rsync/#3-use-configuration-file","title":"3. Use Configuration File","text":"<p>Create an rsync configuration file:</p> <pre><code># ~/.rsync_cluster\n--archive\n--verbose\n--compress\n--progress\n--human-readable\n--exclude=.git\n--exclude=__pycache__\n--exclude=*.tmp\n</code></pre> <p>Use it with: <pre><code>rsync --config=~/.rsync_cluster myproject/ username@cluster.example.com:~/myproject/\n</code></pre></p>"},{"location":"transfer/Rsync/#troubleshooting","title":"Troubleshooting","text":""},{"location":"transfer/Rsync/#permission-denied","title":"Permission Denied","text":"<pre><code># Check SSH connection first\nssh username@cluster.example.com\n\n# Verify destination directory exists and is writable\nssh username@cluster.example.com \"ls -la ~/target_directory\"\n</code></pre>"},{"location":"transfer/Rsync/#slow-transfer-speed","title":"Slow Transfer Speed","text":"<pre><code># Adjust compression level (0-9, where 9 is maximum compression)\nrsync -avz --compress-level=6 myproject/ username@cluster.example.com:~/myproject/\n\n# Use different SSH cipher for faster transfers\nrsync -avz -e \"ssh -c aes128-ctr\" myproject/ username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#large-file-handling","title":"Large File Handling","text":"<pre><code># For very large files, use minimal compression\nrsync -av --compress-level=1 -P largefile.dat username@cluster.example.com:~/data/\n\n# Or disable compression entirely\nrsync -avP largefile.dat username@cluster.example.com:~/data/\n</code></pre>"},{"location":"transfer/Rsync/#monitoring-transfer-progress","title":"Monitoring Transfer Progress","text":""},{"location":"transfer/Rsync/#real-time-progress","title":"Real-time Progress","text":"<pre><code># Show detailed progress with file names\nrsync -avz --progress myproject/ username@cluster.example.com:~/myproject/\n\n# Show overall progress\nrsync -avzP myproject/ username@cluster.example.com:~/myproject/\n</code></pre>"},{"location":"transfer/Rsync/#log-transfer-details","title":"Log Transfer Details","text":"<pre><code># Create detailed log\nrsync -avz --log-file=rsync.log myproject/ username@cluster.example.com:~/myproject/\n\n# View log\ntail -f rsync.log\n</code></pre>"},{"location":"transfer/Rsync/#security-considerations","title":"Security Considerations","text":"<ul> <li>Always use SSH for remote transfers (rsync over SSH is the default)</li> <li>Verify fingerprints when connecting to new clusters</li> <li>Use SSH keys instead of passwords for automated transfers</li> <li>Be cautious with <code>--delete</code> option to avoid accidental data loss</li> <li>Consider using <code>--dry-run</code> first for critical transfers</li> </ul>"},{"location":"transfer/Rsync/#conclusion","title":"Conclusion","text":"<p>Rsync is an essential tool for efficient file transfers to computing clusters. Its incremental transfer capabilities, combined with compression and resumption features, make it ideal for scientific computing workflows where large datasets and frequent synchronization are common.</p> <p>Remember to always test with <code>--dry-run</code> first, especially when using <code>--delete</code>, and consider creating transfer scripts for frequently used commands.</p>"},{"location":"transfer/SFTP/","title":"Transfering Files Using FileZilla and SFTP","text":"Table of Content <ul> <li>Transfering Files Using FileZilla and SFTP<ul> <li>Prerequisites</li> <li>Installing FileZilla</li> <li>Configuring SFTP Connection</li> <li>SSH Key Authentication</li> <li>FileZilla Interface</li> <li>Transferring Files</li> <li>Best Practices</li> <li>Common Troubleshooting</li> <li>Alternative SFTP Commands</li> <li>Additional Resources</li> </ul> </li> </ul> <p>This guide explains how to transfer files from your local computer to a computing cluster using FileZilla and the SFTP protocol (SSH File Transfer Protocol).</p>"},{"location":"transfer/SFTP/#prerequisites","title":"Prerequisites","text":"<p>Before getting started, make sure you have:</p> <ul> <li>FileZilla Client installed on your computer</li> <li>Connection credentials for the cluster (username, password, or SSH key)</li> <li>Server address of the computing cluster</li> <li>Connection port (typically 22 for SFTP)</li> </ul>"},{"location":"transfer/SFTP/#installing-filezilla","title":"Installing FileZilla","text":""},{"location":"transfer/SFTP/#windows","title":"Windows","text":"<ol> <li>Download FileZilla Client from the official website: https://filezilla-project.org/</li> <li>Run the installer and follow the installation instructions</li> <li>Launch FileZilla from the Start menu</li> </ol>"},{"location":"transfer/SFTP/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code>sudo apt update\nsudo apt install filezilla\n</code></pre>"},{"location":"transfer/SFTP/#linux-centosrhelfedora","title":"Linux (CentOS/RHEL/Fedora)","text":"<pre><code># CentOS/RHEL\nsudo yum install filezilla\n\n# Fedora\nsudo dnf install filezilla\n</code></pre>"},{"location":"transfer/SFTP/#macos","title":"macOS","text":"<ol> <li>Download FileZilla Client from the official website</li> <li>Open the downloaded <code>.dmg</code> file</li> <li>Drag FileZilla to the Applications folder</li> <li>Launch FileZilla from Launchpad</li> </ol>"},{"location":"transfer/SFTP/#configuring-sftp-connection","title":"Configuring SFTP Connection","text":""},{"location":"transfer/SFTP/#method-1-quick-connect","title":"Method 1: Quick Connect","text":"<ol> <li>Open FileZilla</li> <li>In the quick connect bar at the top, enter:</li> <li>Host: <code>sftp://cluster-address.example.com</code></li> <li>Username: your username</li> <li>Password: your password</li> <li>Port: 22 (or the port specified by your administrator)</li> <li>Click Quickconnect</li> </ol>"},{"location":"transfer/SFTP/#method-2-site-manager-recommended","title":"Method 2: Site Manager (recommended)","text":"<ol> <li>Go to File \u2192 Site Manager</li> <li>Click New Site</li> <li>Give your connection a name (e.g., \"Computing Cluster\")</li> <li>Configure the settings:</li> <li>Protocol: SFTP - SSH File Transfer Protocol</li> <li>Host: cluster-address.example.com</li> <li>Port: 22</li> <li>Logon Type: <ul> <li>Normal (username + password)</li> <li>Key file (if using SSH private key)</li> </ul> </li> <li>User: your username</li> <li> <p>Password: your password (if using normal authentication)</p> </li> <li> <p>Click Connect</p> </li> </ol> <p></p>"},{"location":"transfer/SFTP/#ssh-key-authentication","title":"SSH Key Authentication","text":"<p>If your cluster uses SSH key authentication:</p> <ol> <li>In the Site Manager, select Key file as the logon type</li> <li>Click Browse next to \"Key file\"</li> <li>Select your SSH private key (typically <code>~/.ssh/id_rsa</code> or <code>~/.ssh/id_ed25519</code>)</li> <li>If your key is not in PPK format, FileZilla will offer to convert it automatically</li> </ol>"},{"location":"transfer/SFTP/#filezilla-interface","title":"FileZilla Interface","text":"<p>Once connected, the FileZilla interface is divided into several sections:</p> <ul> <li>Local panel (left): files and folders on your computer</li> <li>Remote panel (right): files and folders on the cluster</li> <li>Message window (top): connection and transfer status</li> <li>Transfer queue (bottom): transfer queue</li> </ul> <p></p>"},{"location":"transfer/SFTP/#transferring-files","title":"Transferring Files","text":""},{"location":"transfer/SFTP/#simple-transfer","title":"Simple Transfer","text":"<ol> <li>Navigate through your local folders (left panel)</li> <li>Navigate to the destination directory on the cluster (right panel)</li> <li>Drag and drop files from the local panel to the remote panel</li> <li>Alternatively, right-click on a file \u2192 Upload</li> </ol>"},{"location":"transfer/SFTP/#folder-transfer","title":"Folder Transfer","text":"<p>To transfer an entire folder:</p> <ol> <li>Select the folder in the local panel</li> <li>Drag it to the remote panel</li> <li>FileZilla will automatically transfer all contents</li> </ol>"},{"location":"transfer/SFTP/#monitoring-transfers","title":"Monitoring Transfers","text":"<p>The transfer queue window allows you to:</p> <ul> <li>View progress of ongoing transfers</li> <li>Pause or cancel transfers</li> <li>Resume interrupted transfers</li> </ul>"},{"location":"transfer/SFTP/#best-practices","title":"Best Practices","text":""},{"location":"transfer/SFTP/#security","title":"Security","text":"<ul> <li>Always use SFTP instead of FTP for secure connections</li> <li>Verify the server fingerprint on first connection</li> <li>Use SSH keys instead of passwords when possible</li> <li>Close the connection after use</li> </ul>"},{"location":"transfer/SFTP/#performance","title":"Performance","text":"<ul> <li>Adjust the number of simultaneous connections in Edit \u2192 Settings \u2192 Transfers</li> <li>Use compression for large text files</li> <li>Avoid transferring many small files; prefer archives</li> </ul>"},{"location":"transfer/SFTP/#organization","title":"Organization","text":"<ul> <li>Create profiles in the Site Manager for different clusters</li> <li>Use bookmarks to mark frequently used directories</li> <li>Synchronize directories with the synchronization feature</li> </ul>"},{"location":"transfer/SFTP/#common-troubleshooting","title":"Common Troubleshooting","text":""},{"location":"transfer/SFTP/#connection-failure","title":"Connection Failure","text":"<ul> <li>Verify the server address and port</li> <li>Confirm your credentials</li> <li>Check that SSH service is active on the cluster</li> <li>Contact the system administrator if necessary</li> </ul>"},{"location":"transfer/SFTP/#slow-transfers","title":"Slow Transfers","text":"<ul> <li>Check your internet connection</li> <li>Adjust the number of simultaneous connections</li> <li>Use compression for appropriate files</li> </ul>"},{"location":"transfer/SFTP/#permission-errors","title":"Permission Errors","text":"<ul> <li>Verify that you have write permissions in the destination directory</li> <li>Contact the administrator to adjust permissions if necessary</li> </ul>"},{"location":"transfer/SFTP/#alternative-sftp-commands","title":"Alternative SFTP Commands","text":"<p>For advanced users, here are some command-line SFTP commands:</p> <pre><code># SFTP connection\nsftp username@cluster-address.example.com\n\n# Navigation\nls                    # List remote files\nlls                   # List local files\ncd /path/to/directory # Change remote directory\nlcd /local/path       # Change local directory\n\n# Transfer\nput localfile.txt     # Upload a local file\nget remotefile.txt    # Download a remote file\nput -r local_folder   # Upload a folder (recursive)\nget -r remote_folder  # Download a folder (recursive)\n\n# Exit\nquit\n</code></pre>"},{"location":"transfer/SFTP/#additional-resources","title":"Additional Resources","text":"<ul> <li>Official FileZilla Documentation</li> <li>SSH and SFTP Guide</li> <li>Contact your system administrator for specific configurations</li> </ul>"}]}